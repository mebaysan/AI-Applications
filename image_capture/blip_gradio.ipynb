{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65b2cd3-fccb-4649-b7ae-b138204cf553",
   "metadata": {},
   "source": [
    "# BLIP (Bootstrapping Language-Image Pretraining)\n",
    "\n",
    "## Introduction to BLIP\n",
    "\n",
    "BLIP represents a significant advancement in the intersection of natural language processing (NLP) and computer vision. BLIP, designed to improve AI models, enhances their ability to understand and generate image descriptions. It learns to associate images with relevant text, allowing it to generate captions, answer image-related questions, and support image-based search queries.\n",
    "\n",
    "## Why BLIP Matters\n",
    "\n",
    "BLIP is crucial for several reasons:\n",
    "\n",
    "- Enhanced understanding: It provides a more nuanced understanding of the content within images, going beyond object recognition to comprehend scenes, actions, and interactions.\n",
    "- Multimodal learning: By integrating text and image data, BLIP facilitates multimodal learning, which is closer to how humans perceive the world.\n",
    "- Accessibility: Generating accurate image descriptions can make content more accessible to people with visual impairments.\n",
    "- Content creation: It supports creative and marketing endeavors by generating descriptive texts for visual content, saving time and enhancing creativity.\n",
    "\n",
    "## Real-Time Use Case: Automated Photo Captioning\n",
    "\n",
    "A practical application of BLIP is in developing an automated photo captioning system. Such a system can be used in diverse domains. It enhances social media platforms by suggesting captions for uploaded photos automatically. It also aids digital asset management systems by offering searchable descriptions for stored images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adeafc-bbde-49fe-b44f-0dfc768e4ba0",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5296f9be-275f-42e3-b166-2bc0a4c91631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the transformers library\n",
    "#!pip install transformers\n",
    "#!pip install pillow\n",
    "#!pip install torch\n",
    "#!pip install requests\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adcc13d0-6c7f-4b75-baa2-ee2ec44e0ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 14:51:21.934118: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 14:51:23.017448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd9091713f64bfbace5e9bff0215d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a371abb3309f4f4aaad5213756578789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the processor and model from Hugging Face\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b0e100-62e5-44f4-a219-ea524c92df05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerberoos/Workspace/IBM_AIApplications/venv/lib/python3.12/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load an image\n",
    "image = Image.open(\"./resources/BaysanSoft.png\")\n",
    "# Prepare the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "# Generate captions\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81838eae-4bb9-4c44-9155-4356278db9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: the logo for the future of software\n"
     ]
    }
   ],
   "source": [
    " print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be391f-73e4-4dd5-ba2a-a33001a670ae",
   "metadata": {},
   "source": [
    "# Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591470d7-0c19-43c8-b06c-8d574c8db45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4127ebfe-d5fa-442d-9bcc-42382f27759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "img_url = './resources/logo.png' \n",
    "raw_image = Image.open(img_url).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fecdc-e405-499d-acea-b1f680cf7d23",
   "metadata": {},
   "source": [
    "## Conditional Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7848a77-9848-4e7e-b37f-88168e785ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kerberoos/Workspace/IBM_AIApplications/venv/lib/python3.12/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a white snake with a blue tail and a purple tail\n"
     ]
    }
   ],
   "source": [
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c386e2-c52f-40e5-8716-97df704affc3",
   "metadata": {},
   "source": [
    "## Unconditional Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fea805-9052-4e17-adb5-15c401f4e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ad263-2ced-42e9-babe-cf5802cf314d",
   "metadata": {},
   "source": [
    "## My Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "041ae42c-592a-4ec0-b961-c115f83f9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_of_image(image_path, model, processor):\n",
    "    raw_image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_length=150)\n",
    "    summary = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20181df3-4186-42d1-ac57-6052133e8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = Path(os.path.abspath(\".\")) / \"resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9aabb43d-4ea9-4f44-8e13-8fbc29a92f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(str(images_folder / \"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac842b6a-b1f1-4163-9448-58d811f329bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arafed building with a clock tower in the background\n",
      "a large explosion of smoke and smoke is seen from a building\n",
      "zebras and elephants are grazing in a field with a lion\n",
      "a drawing of a lion and a dog are standing together\n"
     ]
    }
   ],
   "source": [
    "for i in images:\n",
    "    summary = summary_of_image(i, model, processor)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da6dd8-6ea5-405b-9918-617f6fe04914",
   "metadata": {},
   "source": [
    "# Gradio\n",
    "\n",
    "## Why Use Gradio?\n",
    "\n",
    "Gradio is useful for several reasons:\n",
    "- Ease of use: Gradio enables the creation of interfaces for models with just a few lines of code.\n",
    "- Flexibility: Gradio supports various inputs and outputs, such as text, images, files, and more.\n",
    "- Sharing and collaboration: Interfaces can be shared with others through unique URLs, facilitating easy collaboration and feedback collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe3fca-4660-4083-86f0-2760dcbc8103",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e41415-c5c1-4921-b20e-82051af53dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694711e3-67e3-44fe-9713-20f2cdc85584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(name, intensity):\n",
    "  return \"Hello, \" + name + \"!\" * int(intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec1d03-646d-476c-a761-05d4fceb7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "  fn=greet,\n",
    "  inputs=[\"text\", \"slider\"],\n",
    "  outputs=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c053d52-23f1-4d2e-98a6-57599235d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch() # http://localhost:7860"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fd952-ad23-4744-999e-d4ac4700c21e",
   "metadata": {},
   "source": [
    "## Understanding the Interface class\n",
    "\n",
    "Note that to make your first demo, you created an instance of the gr.Interface class. The Interface class is designed to create demos for machine learning models that accept one or more inputs and return one or more outputs.\n",
    "\n",
    "The Interface class has three core arguments:\n",
    "\n",
    "- fn: The function to wrap a user interface (UI) around\n",
    "- inputs: The Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n",
    "- outputs: The Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n",
    "\n",
    "The fn argument is flexible — you can pass any Python function you want to wrap with a UI. In the example above, you saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n",
    "\n",
    "The input and output arguments take one or more Gradio components. As we'll see, Gradio includes more than 30 built-in components (such as the gr.Textbox(), gr.Image(), and gr.HTML() components) that are designed for machine learning applications.\n",
    "\n",
    "If your function accepts more than one argument, as is the case above, pass a list of input components to inputs, with each input component corresponding to one of the function's arguments in order. The same applies if your function returns more than one value: simply pass a list of components to outputs. This flexibility makes the Interface class a very powerful way to create demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd5b28-cc6b-4e78-ac36-c4ef82f2efcf",
   "metadata": {},
   "source": [
    "# BLIP + Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1eb2c0-1790-4e5c-933e-da5ccfb429dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed5969-3448-465f-9daa-8b768b3a54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3593fc-64cb-4b43-bfef-4f9cc5e91e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "    \n",
    "def caption_image(image):\n",
    "    \"\"\"\n",
    "    Takes an image input and returns a caption.\n",
    "    \"\"\"\n",
    "    caption = generate_caption(image)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868295a-2e10-4139-9e59-5e9230f86be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=caption_image,\n",
    "    inputs=gr.inputs.Image(type=\"pil\", shape=(224, 224)),\n",
    "    outputs=\"text\",\n",
    "    title=\"Image Captioning with BLIP\",\n",
    "    description=\"Upload an image to generate a caption.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bbeb7-3d8a-4c82-81ef-2bcab192a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
